{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Pre Training\n",
    "\n",
    "### Idea \n",
    "\n",
    "Train a classification model using human radiograph images, and then fine tune on veterinary medical images\n",
    "\n",
    "For the dataset, labels were extracted from the radiologist report where: blank for unmentioned, 0 for negative, -1 for uncertain, and 1 for positive. See [here](https://stanfordmlgroup.github.io/competitions/chexpert/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from keras_cv_attention_models import coatnet\n",
    "from keras_cv_attention_models import swin_transformer_v2\n",
    "from keras_cv_attention_models import nfnets\n",
    "from keras_cv_attention_models import maxvit\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from data_preparation import prepare_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other models to try:\n",
    "\n",
    "- CAFormer\n",
    "- EfficientNetV2M\n",
    "- VOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below can be used to verify that the gpu is in use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TARGET_HEIGHT = 512\n",
    "TARGET_WIDTH = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, train_labels_df, targets = prepare_data(split='train')\n",
    "X_val, y_val, valid_labels_df, _ = prepare_data(split='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_labels_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "valid_labels_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chexnet_targets = ['No Finding',\n",
    "       'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity',\n",
    "       'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis',\n",
    "       'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture',\n",
    "       'Support Devices']\n",
    "\n",
    "chexpert_targets = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Pleural Effusion']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncertainty Approaches\n",
    "The CheXpert paper outlines several different approaches to mapping using the uncertainty labels in the data:\n",
    "\n",
    "- Ignoring - essentially removing from the calculation in the loss function\n",
    "- Binary mapping - sending uncertain values to either 0 or 1\n",
    "- Prevalence mapping - use the rate of prevelance of the feature as it's target value\n",
    "- Self-training - consider the uncertain values as unlabeled\n",
    "- 3-Class Classification - retain a separate value for uncertain and try to predict it as a class in its own right\n",
    "\n",
    "The paper gives the results of different experiments with the above approaches and indicates the most accurate approach for each feature.\n",
    "    \n",
    "|Approach/Feature|Atelectasis|Cardiomegaly|Consolidation|Edema|PleuralEffusion|\n",
    "|-----------|-----------|-----------|-----------|-----------|-----------|\n",
    "|`U-Ignore`|0.818(0.759,0.877)|0.828(0.769,0.888)|0.938(0.905,0.970)|0.934(0.893,0.975)|0.928(0.894,0.962)|\n",
    "|`U-Zeros`|0.811(0.751,0.872)|0.840(0.783,0.897)|0.932(0.898,0.966)|0.929(0.888,0.970)|0.931(0.897,0.965)|\n",
    "|`U-Ones`|**0.858(0.806,0.910)**|0.832(0.773,0.890)|0.899(0.854,0.944)|0.941(0.903,0.980)|0.934(0.901,0.967)|\n",
    "|`U-Mean`|0.821(0.762,0.879)|0.832(0.771,0.892)|0.937(0.905,0.969)|0.939(0.902,0.975)|0.930(0.896,0.965)|\n",
    "|`U-SelfTrained`|0.833(0.776,0.890)|0.831(0.770,0.891)|0.939(0.908,0.971)|0.935(0.896,0.974)|0.932(0.899,0.966)|\n",
    "|`U-MultiClass`|0.821(0.763,0.879)|**0.854(0.800,0.909)**|0.937(0.905,0.969)|0.928(0.887,0.968)|0.936(0.904,0.967)|\n",
    "\n",
    "The binary mapping approaches (U-Ones and U-Zeros) are easiest to implement and so to begin with we take the best option between U-Ones and U-Zeros for each feature\n",
    "\n",
    "- Atelectasis `U-Ones`\n",
    "- Cardiomegaly `U-Zeros`\n",
    "- Consolidation `U-Zeros`\n",
    "- Edema `U-Ones`\n",
    "- Pleural Effusion `U-Zeros`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_df['valid'] = False\n",
    "valid_labels_df['valid'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.concat([train_labels_df, valid_labels_df])\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View a sample of images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the first 5 images\n",
    "paths =  full_df.path[:5]\n",
    "labels = full_df.feature_string[:5]\n",
    "\n",
    "fig, m_axs = plt.subplots(1, len(labels), figsize = (20, 10))\n",
    "#show the images and label them\n",
    "for ii, c_ax in enumerate(m_axs):\n",
    "    c_ax.imshow(np.asarray(Image.open(paths[ii])), cmap='gray')\n",
    "    c_ax.set_title(labels[ii])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = np.asarray(Image.open(X_val[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.stack((img,)*3, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "ax.imshow(img)\n",
    "ax.axis('off')\n",
    "ax.set_aspect('auto')\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmented Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_mask(image, size=12, n_squares=1):\n",
    "    h, w, channels = image.shape\n",
    "    new_image = np.asarray(image.copy())\n",
    "    for _ in range(n_squares):\n",
    "        y = np.random.randint(h)\n",
    "        x = np.random.randint(w)\n",
    "        y1 = np.clip(y - size // 2, 0, h)\n",
    "        y2 = np.clip(y + size // 2, 0, h)\n",
    "        x1 = np.clip(x - size // 2, 0, w)\n",
    "        x2 = np.clip(x + size // 2, 0, w)\n",
    "        new_image[y1:y2, x1:x2, :] = 0\n",
    "    return new_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if np.random.uniform() < 0.5:\n",
    "    augmented = apply_mask(img, size=np.random.randint(low=70, high=240), n_squares=np.random.randint(low=2, high=12))\n",
    "else:\n",
    "    augmented = tf.image.random_brightness(img, max_delta=0.2)\n",
    "    augmented = tf.image.random_saturation(image=augmented, lower=0.8, upper=1.2)\n",
    "    augmented = tf.image.random_hue(image=augmented, max_delta=0.03)\n",
    "    augmented = tf.image.random_contrast(image=augmented, lower=0.8, upper=1.2)\n",
    "# augmented = tf.image.random_flip_up_down(img)\n",
    "# augmented = tf.image.random_flip_left_right(img)\n",
    "# augmented = tf.image.random_saturation(image=img, lower=0.7, upper=1.3)\n",
    "# augmented = tf.image.random_hue(image=img, max_delta=0.03)\n",
    "# augmented = tf.image.random_contrast(image=img, lower=0.7, upper=1.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "augmented.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "ax.imshow(augmented, cmap='gray')\n",
    "ax.axis('off')\n",
    "ax.set_aspect('auto')\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_TRAIN = len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine class weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_columns = [col_name + '_label' for col_name in targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xticks(rotation=90)\n",
    "plt.bar(x=target_columns, height=y_train.sum(axis=0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = y_train.sum(axis=0)\n",
    "total_count = y_train.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_weights = {i: total_count/class_i_count for i, class_i_count in enumerate(class_counts)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_weights_sqrt = {i: np.sqrt(weight) for i, weight in enumerate(list(cls_weights.values()))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_weights_sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_weights_log = {i: np.log(weight) for i, weight in enumerate(list(cls_weights.values()))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_weights_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_weighted = y_train * np.array(list(cls_weights.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xticks(rotation=90)\n",
    "plt.bar(x=target_columns, height=y_train_weighted.sum(axis=0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_weighted_sqrt = y_train * np.array(list(cls_weights_sqrt.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xticks(rotation=90)\n",
    "plt.bar(x=target_columns, height=y_train_weighted_sqrt.sum(axis=0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_weighted_log = y_train * np.array(list(cls_weights_log.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xticks(rotation=90)\n",
    "plt.bar(x=target_columns, height=y_train_weighted_log.sum(axis=0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "USE_CLASS_WEIGHTS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if USE_CLASS_WEIGHTS:\n",
    "    CLASS_WEIGHTS = cls_weights_log\n",
    "else:\n",
    "    CLASS_WEIGHTS = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define dataset generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_image_to_array(path):\n",
    "    img = np.asarray(Image.open(path), dtype=np.float32)\n",
    "    img = np.stack((img,)*3, axis=-1)\n",
    "    img /= 255.\n",
    "    img = tf.image.resize_with_pad(img, target_height=TARGET_HEIGHT, target_width=TARGET_WIDTH)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_model_file(X_path, y):\n",
    "    \"\"\"\n",
    "    X_path: (pandas series) contains the file paths to the images\n",
    "    y: (pandas series of type int) the target label\n",
    "    \n",
    "    return a pair of numpy arrays representing (features, target)\n",
    "    \"\"\"\n",
    "    \n",
    "    X = pd.Series(X_path).apply(convert_image_to_array)\n",
    "    X = X.values\n",
    "    X = list(X)\n",
    "    X = np.array(X, dtype='float32')\n",
    "    \n",
    "    return (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def model_predict(path, model):\n",
    "    x = convert_image_to_array(path=path)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    return model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_data = create_model_file(X_path=X_val, y=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Image.fromarray(np.uint8(255 * val_data[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_image(img_path, target_image_size, dtype, scale_image):\n",
    "    # read the image\n",
    "    img = np.asarray(Image.open(img_path), dtype=dtype)\n",
    "    img = np.stack((img,)*3, axis=-1)\n",
    "\n",
    "    # add image augmentation\n",
    "    if np.random.uniform() < 0.5:\n",
    "        img = apply_mask(img, size=np.random.randint(low=70, high=240), n_squares=np.random.randint(low=2, high=12))\n",
    "    else:\n",
    "        if np.random.uniform() < 0.15:\n",
    "            img = tf.image.random_brightness(img, max_delta=0.2)\n",
    "        if np.random.uniform() < 0.15:\n",
    "            img = tf.image.random_saturation(image=img, lower=0.8, upper=1.2)\n",
    "        if np.random.uniform() < 0.15:\n",
    "            img = tf.image.random_hue(image=img, max_delta=0.03)\n",
    "        if np.random.uniform() < 0.15:\n",
    "            img = tf.image.random_contrast(image=img, lower=0.8, upper=1.2)\n",
    "\n",
    "    if scale_image:\n",
    "        img = img/255.\n",
    "\n",
    "    # resize image\n",
    "    img = tf.image.resize_with_pad(img, target_height=target_image_size[0], target_width=target_image_size[1])\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize some transformed images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = X_val[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = transform_image(img_path=img_path, target_image_size=(TARGET_HEIGHT, TARGET_WIDTH), dtype=np.float32, scale_image=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(np.uint8(255 * img.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(X, y, batch_size, image_size=(TARGET_HEIGHT, TARGET_WIDTH), dtype=np.float32, scale_image=True):\n",
    "    # Get total number of samples in the data\n",
    "    n = len(X)\n",
    "    steps = n//batch_size\n",
    "    \n",
    "    # Define two numpy arrays for containing batch data and labels\n",
    "    batch_data = np.zeros((batch_size, image_size[0], image_size[1], 3), dtype=dtype)\n",
    "    batch_labels = np.zeros((batch_size, num_classes), dtype=dtype)\n",
    "\n",
    "    # Get a numpy array of all the indices of the input data\n",
    "    indices = np.arange(n)\n",
    "    \n",
    "    # Initialize a counter\n",
    "    i = 0\n",
    "    while True:\n",
    "        np.random.shuffle(indices)\n",
    "        # Get the next batch \n",
    "        count = 0\n",
    "        next_batch = indices[(i*batch_size):(i+1)*batch_size]\n",
    "        for j, idx in enumerate(next_batch):\n",
    "            img_path = X[idx]\n",
    "            label = y[idx]\n",
    "            \n",
    "            # one hot encoding\n",
    "            encoded_label = label\n",
    "            \n",
    "            # Transform/augment the image\n",
    "            img = transform_image(img_path=img_path, target_image_size=image_size, dtype=dtype, scale_image=scale_image)\n",
    "            \n",
    "            batch_data[count] = img\n",
    "            batch_labels[count] = encoded_label\n",
    "\n",
    "            count+=1\n",
    "\n",
    "            if count==batch_size:\n",
    "                break\n",
    "            \n",
    "        i+=1\n",
    "        yield batch_data, batch_labels\n",
    "            \n",
    "        if i>=steps:\n",
    "            i=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Utility Functions\n",
    "\n",
    "Define some functions that will help simplify the fine-tuning pre-trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def freeze_layers(model, freeze_layer_name):\n",
    "    for layer in model.layers:\n",
    "        if layer.name != freeze_layer_name:\n",
    "            layer.trainable = False\n",
    "        else:\n",
    "            layer.trainable = False\n",
    "            break\n",
    "            \n",
    "def unfreeze_batch_norm(model):\n",
    "    for layer in model.layers:\n",
    "        if layer.__class__.__name__ == 'BatchNormalization':\n",
    "            layer.trainable = True\n",
    "            \n",
    "def unfreeze_layer_norm(model):\n",
    "    for layer in model.layers:\n",
    "        if layer.__class__.__name__ == 'LayerNormalization':\n",
    "            layer.trainable = True\n",
    "\n",
    "def print_layer_trainable(model):\n",
    "    for layer in model.layers:\n",
    "        print('{0}:\\t{1}'.format(layer.trainable, layer.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all models\n",
    "!ls ../models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'maxvit_base_512_imagenet21k-ft1k.h5'\n",
    "model_path = f'../models/{MODEL_NAME}'\n",
    "model = tf.keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = swin_transformer_v2.SwinTransformerV2Base_window24(input_shape=(TARGET_HEIGHT, TARGET_WIDTH, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = coatnet.CoAtNet2(input_shape=(TARGET_HEIGHT, TARGET_WIDTH, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = nfnets.ECA_NFNetL3(input_shape=(TARGET_HEIGHT, TARGET_WIDTH, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = maxvit.MaxViT_Small(input_shape=(TARGET_HEIGHT, TARGET_WIDTH, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.layers[-6].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.count_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Model (alternative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model_path='./serialized_models/pretrain_model_ConvNeXtBase_w_ClssWgt_01-0.3887.h5'\n",
    "# model_path='./serialized_models/pretrain_model_ConvNeXtBase_w_ClssWgt_01-0.4021.h5'\n",
    "# # model_path='./serialized_models/pretrain_model_ConvNeXtSmall_w_ClssWgt_03-0.5216.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.applications.convnext import LayerScale\n",
    "# model = tf.keras.models.load_model(model_path, custom_objects={'LayerScale': LayerScale})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine where to freeze and cut off base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Note: these were used to train SwinTransformerV2Base_window24\n",
    "# transfer_layer_name = 'pre_output_ln'\n",
    "# transfer_layer = model.get_layer(transfer_layer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Note: these were used to train CoAtNet2\n",
    "# transfer_layer_name = 'stack_4_block_2_ffn_output'\n",
    "# transfer_layer = model.get_layer(transfer_layer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Note: these were used to train ECA_NFNetL3\n",
    "# transfer_layer_name = 'post_swish'\n",
    "# transfer_layer = model.get_layer(transfer_layer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Note: these were used to train MaxViT_Small\n",
    "transfer_layer_name = 'stack_4_block_2/grid_ffn_output'\n",
    "transfer_layer = model.get_layer(transfer_layer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conv_model = tf.keras.Model(inputs=model.input, outputs=transfer_layer.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model(base_model, num_classes, pooling='avg', final_conv_layer='vgg_separable', expand_model=True, dropout_rate=0):\n",
    "    # Get the output of the base model on which we will build\n",
    "    x = base_model.layers[-1].output\n",
    "    \n",
    "    if expand_model:\n",
    "        if final_conv_layer == 'xception':\n",
    "            x = layers.SeparableConv2D(2048, (3, 3), padding='same', use_bias=False, name='block14_sepconv2')(x)\n",
    "            x = layers.BatchNormalization(name='block14_sepconv2_bn')(x)\n",
    "            x = layers.Activation('relu', name='block14_sepconv2_act')(x)\n",
    "            x = keras.layers.Dropout(dropout_rate / 2)(x)\n",
    "        elif final_conv_layer == 'non_separable':\n",
    "            x = layers.Conv2D(2048, (3, 3), padding='same', use_bias=False, name='block14_conv2')(x)\n",
    "            x = layers.BatchNormalization(name='block14_conv2_bn')(x)\n",
    "            x = layers.Activation('relu', name='block14_conv2_act')(x)\n",
    "            x = keras.layers.Dropout(dropout_rate / 2)(x)\n",
    "        elif final_conv_layer == 'vgg_separable':\n",
    "            x = layers.SeparableConv2D(2048, (3,3), activation='relu', padding='same', kernel_regularizer='l1_l2', bias_regularizer='l1_l2', name='block14_sepconv2')(x)\n",
    "            x = keras.layers.Dropout(dropout_rate / 2)(x)\n",
    "        elif final_conv_layer == 'vgg':\n",
    "            x = layers.Conv2D(2048, (3,3), activation='relu', padding='same', name='block14_sepconv2')(x)\n",
    "            x = keras.layers.Dropout(dropout_rate / 2)(x)\n",
    "        else:\n",
    "            raise ValueError('`final_conv_layer` should be one of the following: xception, non_separable, vgg_separable, or vgg')\n",
    "\n",
    "    if pooling == 'global_avg':\n",
    "        x = layers.GlobalAveragePooling2D(name='global_avg_pool')(x)\n",
    "    elif pooling == 'global_max':\n",
    "        x = layers.GlobalMaxPooling2D(name='global_max_pool')(x)\n",
    "    elif pooling == 'max':\n",
    "        x = layers.MaxPooling2D((2,2), name='local_max_pool')(x)\n",
    "        x = layers.Flatten(name='flatten')(x)\n",
    "    elif pooling == 'avg':\n",
    "        x = layers.AveragePooling2D((2,2), name='local_avg_pool')(x)\n",
    "        x = layers.Flatten(name='flatten')(x)\n",
    "    else:\n",
    "        pass\n",
    "    x = keras.layers.Dropout(dropout_rate)(x)\n",
    "        \n",
    "    x = layers.Dense(num_classes, activation='sigmoid', name='prediction')(x)\n",
    "\n",
    "    # Create model.\n",
    "    model = tf.keras.Model(base_model.input, x, name='Xception')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine good starting learning rate\n",
    "\n",
    "Experiment with the proper learning rate range by starting at a low number, see how many epochs for loss to get to a certain value, incrementally increase until the learning rate is too high. Use this range to determine the initial learning rate.\n",
    "\n",
    "Create a function to do this analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_learning_rate(X, y, batch_size: int, lr_list: List[float], steps: int):\n",
    "    train_loss_by_lr = []\n",
    "    \n",
    "    for i, lr in enumerate(lr_list):\n",
    "        print(f'Learning rate {i + 1} of {len(lr_list)}. LR value: {lr}')\n",
    "        local_model = ConvNeXtBase(include_top=False, weights='imagenet', input_tensor=input_tensor, include_preprocessing=False)\n",
    "        local_conv_model = tf.keras.Model(inputs=local_model.input, outputs=local_model.output)\n",
    "        \n",
    "        model_lr = build_model(base_model=local_conv_model, num_classes=num_classes, dropout_rate=0.3)\n",
    "        \n",
    "        model_lr.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "            loss=\"binary_crossentropy\",\n",
    "            metrics=[\"accuracy\"],\n",
    "        )\n",
    "        \n",
    "        hist = model_lr.fit(\n",
    "            x=data_gen(X=X, y=y, batch_size=batch_size), \n",
    "            epochs=1, \n",
    "            steps_per_epoch=steps,\n",
    "            class_weight=CLASS_WEIGHTS,\n",
    "        )\n",
    "        \n",
    "        final_loss = hist.history['loss'][-1]\n",
    "        final_acc = hist.history['accuracy'][-1]\n",
    "        \n",
    "        train_loss_by_lr.append((lr, final_loss, final_acc))\n",
    "      \n",
    "    losses_df = pd.DataFrame(train_loss_by_lr, columns=['learning_rate', 'training_loss', 'training_accuracy'])\n",
    "    \n",
    "    return losses_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_loss_df = determine_learning_rate(X=X_train, \n",
    "#                                      y=y_train, \n",
    "#                                      batch_size=2, \n",
    "#                                      lr_list=[1e-2, 5e-3, 1e-3, 5e-4, 1e-4, 5e-5, 1e-5, 5e-6, 1e-6, 1e-7, 1e-8,],\n",
    "#                                      steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(x=lr_loss_df['learning_rate'].values, y=lr_loss_df['training_loss'].values)\n",
    "# plt.xscale('log')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ideally retrain the entire model, but memory is constrained \n",
    "# freeze_layers(conv_model, freeze_layer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = build_model(base_model=conv_model, num_classes=num_classes, dropout_rate=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unfreeze_batch_norm(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unfreeze_layer_norm(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_layer_trainable(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lion Optimizer\n",
    "\n",
    "[Lion](https://arxiv.org/pdf/2302.06675.pdf) is a new optimizer that helps to converge more quickly to better models with better memory efficiency. The official implementation is [here](https://github.com/google/automl/blob/master/lion/lion_tf2.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lion(tf.keras.optimizers.legacy.Optimizer):\n",
    "  r\"\"\"Optimizer that implements the Lion algorithm.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               learning_rate=0.0001,\n",
    "               beta_1=0.9,\n",
    "               beta_2=0.99,\n",
    "               wd=0,\n",
    "               name='lion',\n",
    "               **kwargs):\n",
    "    \"\"\"Construct a new Lion optimizer.\"\"\"\n",
    "\n",
    "    super(Lion, self).__init__(name, **kwargs)\n",
    "    self._set_hyper('learning_rate', kwargs.get('lr', learning_rate))\n",
    "    self._set_hyper('beta_1', beta_1)\n",
    "    self._set_hyper('beta_2', beta_2)\n",
    "    self._set_hyper('wd', wd)\n",
    "\n",
    "  def _create_slots(self, var_list):\n",
    "    # Create slots for the first and second moments.\n",
    "    # Separate for-loops to respect the ordering of slot variables from v1.\n",
    "    for var in var_list:\n",
    "      self.add_slot(var, 'm')\n",
    "\n",
    "  def _prepare_local(self, var_device, var_dtype, apply_state):\n",
    "    super(Lion, self)._prepare_local(var_device, var_dtype, apply_state)\n",
    "\n",
    "    beta_1_t = tf.identity(self._get_hyper('beta_1', var_dtype))\n",
    "    beta_2_t = tf.identity(self._get_hyper('beta_2', var_dtype))\n",
    "    wd_t = tf.identity(self._get_hyper('wd', var_dtype))\n",
    "    lr = apply_state[(var_device, var_dtype)]['lr_t']\n",
    "    apply_state[(var_device, var_dtype)].update(\n",
    "        dict(\n",
    "            lr=lr,\n",
    "            beta_1_t=beta_1_t,\n",
    "            one_minus_beta_1_t=1 - beta_1_t,\n",
    "            beta_2_t=beta_2_t,\n",
    "            one_minus_beta_2_t=1 - beta_2_t,\n",
    "            wd_t=wd_t))\n",
    "\n",
    "  @tf.function(jit_compile=True)\n",
    "  def _resource_apply_dense(self, grad, var, apply_state=None):\n",
    "    var_device, var_dtype = var.device, var.dtype.base_dtype\n",
    "    coefficients = ((apply_state or {}).get((var_device, var_dtype)) or\n",
    "                    self._fallback_apply_state(var_device, var_dtype))\n",
    "\n",
    "    m = self.get_slot(var, 'm')\n",
    "    var_t = var.assign_sub(\n",
    "        coefficients['lr_t'] *\n",
    "        (tf.math.sign(m * coefficients['beta_1_t'] +\n",
    "                      grad * coefficients['one_minus_beta_1_t']) +\n",
    "         var * coefficients['wd_t']))\n",
    "    with tf.control_dependencies([var_t]):\n",
    "      m.assign(m * coefficients['beta_2_t'] +\n",
    "               grad * coefficients['one_minus_beta_2_t'])\n",
    "\n",
    "  @tf.function(jit_compile=True)\n",
    "  def _resource_apply_sparse(self, grad, var, indices, apply_state=None):\n",
    "    var_device, var_dtype = var.device, var.dtype.base_dtype\n",
    "    coefficients = ((apply_state or {}).get((var_device, var_dtype)) or\n",
    "                    self._fallback_apply_state(var_device, var_dtype))\n",
    "\n",
    "    m = self.get_slot(var, 'm')\n",
    "    m_t = m.assign(m * coefficients['beta_1_t'])\n",
    "    m_scaled_g_values = grad * coefficients['one_minus_beta_1_t']\n",
    "    m_t = m_t.scatter_add(tf.IndexedSlices(m_scaled_g_values, indices))\n",
    "    var_t = var.assign_sub(coefficients['lr'] *\n",
    "                           (tf.math.sign(m_t) + var * coefficients['wd_t']))\n",
    "\n",
    "    with tf.control_dependencies([var_t]):\n",
    "      m_t = m_t.scatter_add(tf.IndexedSlices(-m_scaled_g_values, indices))\n",
    "      m_t = m_t.assign(m_t * coefficients['beta_2_t'] /\n",
    "                       coefficients['beta_1_t'])\n",
    "      m_scaled_g_values = grad * coefficients['one_minus_beta_2_t']\n",
    "      m_t.scatter_add(tf.IndexedSlices(m_scaled_g_values, indices))\n",
    "\n",
    "  def get_config(self):\n",
    "    config = super(Lion, self).get_config()\n",
    "    config.update({\n",
    "        'learning_rate': self._serialize_hyperparameter('learning_rate'),\n",
    "        'beta_1': self._serialize_hyperparameter('beta_1'),\n",
    "        'beta_2': self._serialize_hyperparameter('beta_2'),\n",
    "        'wd': self._serialize_hyperparameter('wd'),\n",
    "    })\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = keras.optimizers.schedules.CosineDecayRestarts(\n",
    "      # initial_learning_rate=3e-3, # for uniform weighting\n",
    "      # initial_learning_rate=3e-4, # for sqrt weighting \n",
    "      initial_learning_rate=1e-5, # lower learning rate for Lion optimizer\n",
    "      first_decay_steps=int(NUM_TRAIN/ (7 * batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cycle through cosine decay with restarts 7 times per epoch\n",
    "print(NUM_TRAIN/ (7 * batch_size))\n",
    "print(int(NUM_TRAIN/ (7 * batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 2\n",
    "model_path='./serialized_models/pretrain_model_MaxViT_w_ClssWgt_{epoch:02d}-{val_loss:.4f}.h5'\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=model_path, save_best_only=True),\n",
    "    tf.keras.callbacks.EarlyStopping(patience=10)\n",
    "]\n",
    "model.compile(\n",
    "#     optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "    optimizer=Lion(learning_rate=lr_schedule),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "model.fit(\n",
    "    x=data_gen(X=X_train, y=y_train, batch_size=batch_size), \n",
    "    epochs=epochs, \n",
    "    callbacks=callbacks, \n",
    "    validation_data=val_data, \n",
    "    validation_batch_size=4,\n",
    "    steps_per_epoch=int(NUM_TRAIN/batch_size),\n",
    "    class_weight=CLASS_WEIGHTS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SwinV2: 1000/111707: loss: 1.446, accuracy: 0.0624\n",
    "SwinV2: 8000/111707: loss: 1.376, accuracy: 0.0392\n",
    "CoAtNet2: 1000/111707: loss: 1.468, accuracy: 0.0750\n",
    "CoAtNet2: 4200/111707: loss: 1.371, accuracy: 0.0840\n",
    "NFNet (w/ Lion): 500/55853: loss: 1.465, accuracy: 0.0319\n",
    "NFNet (w/ Lion): 2100/55853: loss: 1.37, accuracy: 0.0433\n",
    "NFNet (w/ Lion): 4673/55853: loss: 1.33, accuracy: 0.0596\n",
    "MaxViT (w/ Lion): 500/111707: loss: 1.465, accuracy: 0.0319\n",
    "MaxViT (w/ Lion): 5500/111707: loss: 1.247, accuracy: 0.1242\n",
    "MaxViT (w/ Lion): 13500/111707: loss: 1.196, accuracy: 0.15\n",
    "\n",
    "Old convnet: 13300/55853, loss: 0.3006, accuracy: 0.2011"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save final model - Make sure name is correct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save('./serialized_models/pretrain_model_MaxViT_w_ClssWgt_01-0.unk.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
