{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.convnext import LayerScale\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix, f1_score, log_loss, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from data_preparation import prepare_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_HEIGHT = 640\n",
    "TARGET_WIDTH = 640"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, y_val, valid_labels_df, targets = prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the first 5 images\n",
    "paths =  valid_labels_df.path[:5]\n",
    "labels = valid_labels_df.feature_string[:5]\n",
    "\n",
    "fig, m_axs = plt.subplots(1, len(labels), figsize = (20, 10))\n",
    "#show the images and label them\n",
    "for ii, c_ax in enumerate(m_axs):\n",
    "    c_ax.imshow(np.asarray(Image.open(paths[ii])), cmap='gray')\n",
    "    c_ax.set_title(labels[ii])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'pretrain_model_ConvNeXtBase_w_ClssWgt_01-0.3616.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f'../models/{MODEL_NAME}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(model_path, custom_objects={'LayerScale': LayerScale})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store predictions\n",
    "\n",
    "Store all predictions in a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CAUTION! Make sure the preprocessing matches what was used during model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_image_to_array(path):\n",
    "    img = np.asarray(Image.open(path), dtype=np.float32)\n",
    "    img = np.stack((img,)*3, axis=-1)\n",
    "    img /= 255.\n",
    "    img = tf.image.resize_with_pad(img, target_height=TARGET_HEIGHT, target_width=TARGET_WIDTH)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(path, model):\n",
    "    x = convert_image_to_array(path=path)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    return model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_columns = [col_name + '_pred' for col_name in targets]\n",
    "target_columns = [col_name + '_label' for col_name in targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_model_preds = pd.DataFrame(0, index=np.arange(len(X_val)), columns=pred_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, path in enumerate(X_val):\n",
    "    if i % 10 == 0:\n",
    "        print(f'{i} out of {len(X_val)}')\n",
    "    all_model_preds.iloc[i, :] = model_predict(path=path, model=model)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_model_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(data=y_val, columns=target_columns)\n",
    "results = pd.concat([results, all_model_preds], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = results[target_columns].values\n",
    "pred = results[pred_columns].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xticks(rotation=90)\n",
    "plt.bar(x = target_columns, height= y.sum(axis=0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_positives(y, pred, th=0.5):\n",
    "    pred_t = (pred > th)\n",
    "    return np.sum((pred_t == True) & (y == 1))\n",
    "\n",
    "\n",
    "def true_negatives(y, pred, th=0.5):\n",
    "    pred_t = (pred > th)\n",
    "    return np.sum((pred_t == False) & (y == 0))\n",
    "\n",
    "\n",
    "def false_negatives(y, pred, th=0.5):\n",
    "    pred_t = (pred > th)\n",
    "    return np.sum((pred_t == False) & (y == 1))\n",
    "\n",
    "\n",
    "def false_positives(y, pred, th=0.5):\n",
    "    pred_t = (pred > th)\n",
    "    return np.sum((pred_t == True) & (y == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance_metrics(y, pred, class_labels, tp=true_positives,\n",
    "                            tn=true_negatives, fp=false_positives,\n",
    "                            fn=false_negatives,\n",
    "                            acc=None, prevalence=None, spec=None,\n",
    "                            sens=None, ppv=None, npv=None, auc=None, f1=None,\n",
    "                            thresholds=[]):\n",
    "    if len(thresholds) != len(class_labels):\n",
    "        thresholds = [.5] * len(class_labels)\n",
    "\n",
    "    metrics = [tp, tn, fp, fn, acc, prevalence, sens, spec, ppv, npv, auc, f1]\n",
    "    metric_names = [\"TP\", \"TN\", \"FP\", \"FN\", \"Accuracy\", \"Prevalence\",\n",
    "                    \"Sensitivity\", \"Specificity\", \"PPV\", \"NPV\", \"AUC\", \"F1\"]\n",
    "\n",
    "    df = pd.DataFrame(index=class_labels, columns=metric_names + [\"Threshold\"])\n",
    "\n",
    "    for i, label in enumerate(class_labels):\n",
    "        for metric, name in zip(metrics, metric_names):\n",
    "            if metric is not None:\n",
    "                try:\n",
    "                    if name in [\"AUC\", \"F1\"]:\n",
    "                        df.loc[label, name] = round(metric(y[:, i], pred[:, i]), 3)\n",
    "                    elif name == \"Prevalence\":\n",
    "                        df.loc[label, name] = round(metric(y[:, i]), 3)\n",
    "                    else:\n",
    "                        df.loc[label, name] = round(metric(y[:, i], pred[:, i], thresholds[i]), 3)\n",
    "                except Exception as e:\n",
    "                    print(f\"Exception occurred in {name}: {e}\")\n",
    "                    df.loc[label, name] = np.NAN\n",
    "            else:\n",
    "                df.loc[label, name] = \"Not Defined\"\n",
    "        df.loc[label, \"Threshold\"] = round(thresholds[i], 3)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_confidence_intervals(class_labels, statistics):\n",
    "    df = pd.DataFrame(columns=[\"Mean AUC (CI 5%-95%)\"])\n",
    "    for i in range(len(class_labels)):\n",
    "        mean = statistics.mean(axis=1)[i]\n",
    "        max_ = np.quantile(statistics, .95, axis=1)[i]\n",
    "        min_ = np.quantile(statistics, .05, axis=1)[i]\n",
    "        df.loc[class_labels[i]] = [\"%.2f (%.2f-%.2f)\" % (mean, min_, max_)]\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_curve(gt, pred, target_names, curve='roc'):\n",
    "    for i in range(len(target_names)):\n",
    "        if curve == 'roc':\n",
    "            curve_function = roc_curve\n",
    "            auc_roc = roc_auc_score(gt[:, i], pred[:, i])\n",
    "            label = target_names[i] + \" AUC: %.3f \" % auc_roc\n",
    "            xlabel = \"False positive rate\"\n",
    "            ylabel = \"True positive rate\"\n",
    "            a, b, _ = curve_function(gt[:, i], pred[:, i])\n",
    "            plt.figure(1, figsize=(7, 7))\n",
    "            plt.plot([0, 1], [0, 1], 'k--')\n",
    "            plt.plot(a, b, label=label)\n",
    "            plt.xlabel(xlabel)\n",
    "            plt.ylabel(ylabel)\n",
    "\n",
    "            plt.legend(loc='upper center', bbox_to_anchor=(1.3, 1),\n",
    "                       fancybox=True, ncol=1)\n",
    "        elif curve == 'prc':\n",
    "            precision, recall, _ = precision_recall_curve(gt[:, i], pred[:, i])\n",
    "            average_precision = average_precision_score(gt[:, i], pred[:, i])\n",
    "            label = target_names[i] + \" Avg.: %.3f \" % average_precision\n",
    "            plt.figure(1, figsize=(7, 7))\n",
    "            plt.step(recall, precision, where='post', label=label)\n",
    "            plt.xlabel('Recall')\n",
    "            plt.ylabel('Precision')\n",
    "            plt.ylim([0.0, 1.05])\n",
    "            plt.xlim([0.0, 1.0])\n",
    "            plt.legend(loc='upper center', bbox_to_anchor=(1.3, 1),\n",
    "                       fancybox=True, ncol=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(y, pred, th=0.5):\n",
    "    \"\"\"\n",
    "    Compute accuracy of predictions at threshold.\n",
    "\n",
    "    Args:\n",
    "        y (np.array): ground truth, size (n_examples)\n",
    "        pred (np.array): model output, size (n_examples)\n",
    "        th (float): cutoff value for positive prediction from model\n",
    "    Returns:\n",
    "        accuracy (float): accuracy of predictions at threshold\n",
    "    \"\"\"\n",
    "    TP = true_positives(y, pred, th)\n",
    "    FP = false_positives(y, pred, th)\n",
    "    TN = true_negatives(y, pred, th)\n",
    "    FN = false_negatives(y, pred, th)\n",
    "\n",
    "    total = TP + TN + FP + FN\n",
    "    accuracy = (TP + TN) / total if total else 0.0\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "def get_prevalence(y):\n",
    "    \"\"\"\n",
    "    Compute prevalence of positive cases.\n",
    "\n",
    "    Args:\n",
    "        y (np.array): ground truth, size (n_examples)\n",
    "    Returns:\n",
    "        prevalence (float): prevalence of positive cases\n",
    "    \"\"\"\n",
    "    total = len(y)\n",
    "    prevalence = np.sum(y) / total if total else 0.0\n",
    "    \n",
    "    return prevalence\n",
    "\n",
    "def get_sensitivity(y, pred, th=0.5):\n",
    "    \"\"\"\n",
    "    Compute sensitivity of predictions at threshold.\n",
    "\n",
    "    Args:\n",
    "        y (np.array): ground truth, size (n_examples)\n",
    "        pred (np.array): model output, size (n_examples)\n",
    "        th (float): cutoff value for positive prediction from model\n",
    "    Returns:\n",
    "        sensitivity (float): probability that our test outputs positive given that the case is actually positive\n",
    "    \"\"\"\n",
    "    TP = true_positives(y, pred, th)\n",
    "    FN = false_negatives(y, pred, th)\n",
    "\n",
    "    total = TP + FN\n",
    "    sensitivity = TP / total if total else 0.0\n",
    "    \n",
    "    return sensitivity\n",
    "\n",
    "def get_specificity(y, pred, th=0.5):\n",
    "    \"\"\"\n",
    "    Compute specificity of predictions at threshold.\n",
    "\n",
    "    Args:\n",
    "        y (np.array): ground truth, size (n_examples)\n",
    "        pred (np.array): model output, size (n_examples)\n",
    "        th (float): cutoff value for positive prediction from model\n",
    "    Returns:\n",
    "        specificity (float): probability that the test outputs negative given that the case is actually negative\n",
    "    \"\"\"\n",
    "    TN = true_negatives(y, pred, th)\n",
    "    FP = false_positives(y, pred, th)\n",
    "    \n",
    "    total = TN + FP\n",
    "    specificity = TN / total if total else 0.0\n",
    "    \n",
    "    return specificity\n",
    "\n",
    "def get_ppv(y, pred, th=0.5):\n",
    "    \"\"\"\n",
    "    Compute PPV of predictions at threshold.\n",
    "\n",
    "    Args:\n",
    "        y (np.array): ground truth, size (n_examples)\n",
    "        pred (np.array): model output, size (n_examples)\n",
    "        th (float): cutoff value for positive prediction from model\n",
    "    Returns:\n",
    "        PPV (float): positive predictive value of predictions at threshold\n",
    "    \"\"\"\n",
    "    TP = true_positives(y, pred, th)\n",
    "    FP = false_positives(y, pred, th)\n",
    "\n",
    "    total = TP + FP\n",
    "    PPV = TP / total if total else 0.0\n",
    "    \n",
    "    return PPV\n",
    "\n",
    "def get_npv(y, pred, th=0.5):\n",
    "    \"\"\"\n",
    "    Compute NPV of predictions at threshold.\n",
    "\n",
    "    Args:\n",
    "        y (np.array): ground truth, size (n_examples)\n",
    "        pred (np.array): model output, size (n_examples)\n",
    "        th (float): cutoff value for positive prediction from model\n",
    "    Returns:\n",
    "        NPV (float): negative predictive value of predictions at threshold\n",
    "    \"\"\"\n",
    "    TN = true_negatives(y, pred, th)\n",
    "    FN = false_negatives(y, pred, th)\n",
    "\n",
    "    total = TN + FN\n",
    "    NPV = TN / total if total else 0.0\n",
    "    \n",
    "    return NPV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_performance_metrics(y, pred, target_columns, acc=get_accuracy, prevalence=get_prevalence, \n",
    "                        sens=get_sensitivity, spec=get_specificity, ppv=get_ppv, npv=get_npv, auc=roc_auc_score, f1=f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_auc(y, pred, classes, bootstraps=100, fold_size=1000):\n",
    "    statistics = np.full((len(classes), bootstraps), 0.0)\n",
    "\n",
    "    for c in range(len(classes)):\n",
    "        df = pd.DataFrame({'y': y[:, c], 'pred': pred[:, c]})\n",
    "        prevalences = df.groupby('y').size() / len(df)\n",
    "\n",
    "        for i in range(bootstraps):\n",
    "            samples = df.groupby('y').apply(lambda group: group.sample(n=int(fold_size * prevalences[group.name]), replace=True))\n",
    "            y_sample = samples.y.values\n",
    "            pred_sample = samples.pred.values\n",
    "\n",
    "            try:\n",
    "                statistics[c, i] = roc_auc_score(y_sample, pred_sample)\n",
    "            except ValueError:\n",
    "                pass  # Keep default score of 0 if AUC cannot be calculated\n",
    "\n",
    "    return statistics\n",
    "\n",
    "statistics = bootstrap_auc(y, pred, target_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_confidence_intervals(target_columns, statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "def plot_calibration_curve(y, pred):\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    for i in range(len(target_columns)):\n",
    "        plt.subplot(4, 4, i + 1)\n",
    "        fraction_of_positives, mean_predicted_value = calibration_curve(y[:,i], pred[:,i], n_bins=20)\n",
    "        plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "        plt.plot(mean_predicted_value, fraction_of_positives, marker='.')\n",
    "        plt.xlabel(\"Predicted Value\")\n",
    "        plt.ylabel(\"Fraction of Positives\")\n",
    "        plt.title(target_columns[i])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_calibration_curve(y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_performance_df = get_performance_metrics(y, pred, target_columns, acc=get_accuracy, prevalence=get_prevalence, \n",
    "                        sens=get_sensitivity, spec=get_specificity, ppv=get_ppv, npv=get_npv, auc=roc_auc_score,f1=f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_performance_df.to_csv(f'model_performance/test_metrics_{MODEL_NAME}.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
